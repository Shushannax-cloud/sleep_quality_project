{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c891f1ec-4eef-4620-a2f5-f2f9d3fa41d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\User\\.cache\\kagglehub\\datasets\\varishabatool\\disorder\\versions\\1\n",
      "Original Quality of Sleep counts:\n",
      "Quality of Sleep\n",
      "8    109\n",
      "6    105\n",
      "7     77\n",
      "9     71\n",
      "5      7\n",
      "4      5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "path = kagglehub.dataset_download(\"varishabatool/disorder\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "os.listdir(path)\n",
    "# looks inside the folder stored in path, shows all files downloaded by kagglehub.\n",
    "df = pd.read_csv(path + '/Sleep_health_and_lifestyle_dataset.csv')\n",
    "# read the csv file, converst it into a pandas dataframe\n",
    "print(\"Original Quality of Sleep counts:\")\n",
    "print(df['Quality of Sleep'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2be492bc-235e-43df-936d-ee941389371c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution (train): [ 78 146]\n",
      "Class distribution (val): [19 55]\n",
      "Class distribution (test): [20 56]\n"
     ]
    }
   ],
   "source": [
    "# preproccessing\n",
    "# Map original classes to binary\n",
    "binary_mapping = {4:0, 5:0, 6:0, 7:1, 8:1, 9:1}\n",
    "df['Sleep_Binary'] = df['Quality of Sleep'].map(binary_mapping)\n",
    "\n",
    "# Drop rows with NaN (if any)\n",
    "df = df.dropna(subset=['Sleep_Binary'])\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(['Quality of Sleep', 'Sleep_Binary'], axis=1)\n",
    "y = df['Sleep_Binary'].astype(int)\n",
    "\n",
    "# One-hot encode categorical variables if needed\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "X = X.astype(float)\n",
    "\n",
    "# Normalize features\n",
    "X_min = X.min(axis=0)\n",
    "X_max = X.max(axis=0)\n",
    "X_norm = (X - X_min) / (X_max - X_min + 1e-8)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X_norm = X_norm.values\n",
    "y = y.values\n",
    "\n",
    "# Shuffle dataset\n",
    "np.random.seed(42)\n",
    "indices = np.arange(X_norm.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "X_norm = X_norm[indices]\n",
    "y = y[indices]\n",
    "\n",
    "# Split dataset\n",
    "total = X_norm.shape[0]\n",
    "train_size = int(0.6 * total)\n",
    "val_size = int(0.2 * total)\n",
    "\n",
    "X_train = X_norm[:train_size]\n",
    "y_train = y[:train_size]\n",
    "\n",
    "X_val = X_norm[train_size:train_size+val_size]\n",
    "y_val = y[train_size:train_size+val_size]\n",
    "\n",
    "X_test = X_norm[train_size+val_size:]\n",
    "y_test = y[train_size+val_size:]\n",
    "\n",
    "# Check class distribution\n",
    "print(\"Class distribution (train):\", np.bincount(y_train))\n",
    "print(\"Class distribution (val):\", np.bincount(y_val))\n",
    "print(\"Class distribution (test):\", np.bincount(y_test))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "3d066e7e-487e-4faa-9ac0-4c3748cad5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation_accuracy: 0.9864864864864865\n",
      "Test accuracy: 0.9605263157894737\n",
      "Predicted classes (val): [20 54]\n",
      "Predicted classes (test): [23 53]\n",
      "(224, 46)\n"
     ]
    }
   ],
   "source": [
    "# Linear regression\n",
    "class LinearRegression:\n",
    "    def __init__(self, lr = 0.1, epochs = 1000):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.W = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            y_pred = np.dot(X, self.W) + self.b\n",
    "            # compute gradients\n",
    "            dw = (1/n_samples) * np.dot(X.T, (y_pred - y))\n",
    "            db = (1/n_samples) * np.sum(y_pred - y)\n",
    "\n",
    "            # update weights\n",
    "            self.W -= self.lr * dw\n",
    "            self.b -= self.lr * db\n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.W) + self.b\n",
    "\n",
    "linreg = LinearRegression(lr = 0.01, epochs = 2000)\n",
    "linreg.fit(X_train, y_train)\n",
    "\n",
    "y_val_pred_cont = linreg.predict(X_val)\n",
    "y_test_pred_cont = linreg.predict(X_test)\n",
    "\n",
    "y_val_pred = (y_val_pred_cont >= 0.5).astype(int)\n",
    "y_test_pred = (y_test_pred_cont >=0.5).astype(int)\n",
    "\n",
    "print(\"Validation_accuracy:\", np.mean(y_val_pred == y_val))\n",
    "print(\"Test accuracy:\", np.mean(y_test_pred == y_test))\n",
    "\n",
    "print(\"Predicted classes (val):\", np.bincount(y_val_pred))\n",
    "print(\"Predicted classes (test):\", np.bincount(y_test_pred))\n",
    "print(X_train.shape)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1d419a37-e5f2-48f2-bcfe-a93b7e237033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation_accuracy: 0.9864864864864865\n",
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "#  Logistic regression\n",
    "class LogisticRegression:\n",
    "    def __init__(self, lr = 0.1, epochs = 500):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "    def _softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims = True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims = True)\n",
    "    def _one_hot(self, y):\n",
    "        n_classes = np.max(y) +1\n",
    "        one_hot = np.zeros((y.shape[0], n_classes))\n",
    "        one_hot[np.arange(y.shape[0]), y] = 1\n",
    "        return one_hot\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_classes = np.max(y) + 1\n",
    "        self.W = np.zeros((n_features, n_classes))\n",
    "        self.b = np.zeros((1, n_classes))\n",
    "        y_onehot = self._one_hot(y)\n",
    "        for i in range(self.epochs):\n",
    "            z = np.dot(X, self.W) + self.b\n",
    "            y_pred = self._softmax(z)\n",
    "            # gradient\n",
    "            dw = (1/n_samples) * np.dot(X.T, (y_pred - y_onehot))\n",
    "            db = (1/n_samples) * np.sum(y_pred - y_onehot, axis = 0, keepdims = True)\n",
    "            self.W -= self.lr * dw\n",
    "            self.b -= self.lr * db\n",
    "    def predict(self, X):\n",
    "        z = np.dot(X, self.W) + self.b\n",
    "        y_pred = self._softmax(z)\n",
    "        return np.argmax(y_pred, axis = 1)\n",
    "model = LogisticRegression(lr = 0.1, epochs = 1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_val_pred = model.predict(X_val)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Validation_accuracy:\", np.mean(y_val_pred == y_val))\n",
    "print(\"Test accuracy:\", np.mean(y_test_pred == y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "02e4020b-ee41-4ff0-a63f-4bdae142536c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation_accuracy: 0.9864864864864865\n",
      "Test accuracy: 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "# K-Nearest Neighbors\n",
    "class KNN:\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "    def predict(self, X):\n",
    "        y_pred = []\n",
    "        for x in X:\n",
    "            # Euclidean distance\n",
    "            distances = np.sqrt(np.sum((self.X_train - x)**2, axis = 1))\n",
    "            k_idx = np.argsort(distances)[:self.k]\n",
    "            k_labels = self.y_train[k_idx]\n",
    "            # majority vote\n",
    "            counts = np.bincount(k_labels)\n",
    "            y_pred.append(np.argmax(counts))\n",
    "        return np.array(y_pred)\n",
    "knn = KNN(k=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_val_pred = knn.predict(X_val)\n",
    "y_test_pred = knn.predict(X_test)\n",
    "\n",
    "print(\"Validation_accuracy:\", np.mean(y_val_pred == y_val))\n",
    "print(\"Test accuracy:\", np.mean(y_test_pred == y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1b11c765-3a9f-44fd-a323-6853edb620bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation_accuracy: 0.7432432432432432\n",
      "Test accuracy: 0.7368421052631579\n"
     ]
    }
   ],
   "source": [
    "# Decision tree\n",
    "class DecisionTreeNode:\n",
    "    def __init__(self, gini, num_samples, num_samples_per_class, predicted_class):\n",
    "        self.gini = gini\n",
    "        self.num_samples = num_samples\n",
    "        self.num_samples_per_class = num_samples_per_class\n",
    "        self.predicted_class = predicted_class\n",
    "        self.feature_index = None\n",
    "        self.threshold = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth = None):\n",
    "        self.max_depth = max_depth\n",
    "        self.n_classes = None\n",
    "        self.n_features = None\n",
    "        self.tree = None\n",
    "\n",
    "    def _gini(self, y):\n",
    "        m = y.size\n",
    "        if m==0:\n",
    "            return 0\n",
    "        counts = np.bincount(y, minlength = self.n_classes)\n",
    "        prob = counts / m\n",
    "        return 1 - np.sum(prob**2)\n",
    "    def _best_split(self, X, y):\n",
    "        m, n = X.shape\n",
    "        if m <= 1:\n",
    "            return None, None\n",
    "        best_gini = 1.0\n",
    "        best_idx, best_thr = None, None\n",
    "        for idx in range(n):\n",
    "            thresholds, classes = zip(*sorted(zip(X[:, idx], y)))\n",
    "            num_left = np.zeros(self.n_classes)\n",
    "            num_right = np.bincount(classes, minlength = self.n_classes)\n",
    "            for i in range(1, m):\n",
    "                c = classes[i-1]\n",
    "                num_left[c] += 1\n",
    "                num_right[c] -= 1\n",
    "                gini_left = 1 - np.sum((num_left / i) ** 2)\n",
    "                gini_right = 1 - np.sum(num_right / (m-i) ** 2)\n",
    "                gini = (i * gini_left + (m-i) * gini_right) / m\n",
    "                if thresholds[i] == thresholds[i-1]:\n",
    "                    continue\n",
    "                if gini < best_gini:\n",
    "                    best_gini = gini\n",
    "                    best_idx = idx\n",
    "                    best_thr = (thresholds[i] + thresholds[i-1]) / 2\n",
    "\n",
    "        return best_idx, best_thr\n",
    "    def _grow_tree(self, X, y, depth = 0):\n",
    "        num_samples_per_class = [np.sum(y==i) for i in range(self.n_classes)]\n",
    "        predicted_class = np.argmax(num_samples_per_class)\n",
    "        node = DecisionTreeNode(\n",
    "            gini = self._gini(y), \n",
    "            num_samples = y.size, \n",
    "            num_samples_per_class = num_samples_per_class, \n",
    "            predicted_class = predicted_class\n",
    "        )\n",
    "        if depth < self.max_depth:\n",
    "            idx, thr = self._best_split(X, y)\n",
    "            if idx is not None:\n",
    "                indices_left = X[:, idx] <=thr\n",
    "                X_left, y_left = X[indices_left], y[indices_left]\n",
    "                X_right, y_right = X[~indices_left], y[~indices_left]\n",
    "                node.feature_index = idx\n",
    "                node.threshold = thr\n",
    "                node.left = self._grow_tree(X_left, y_left, depth + 1)\n",
    "                node.right = self._grow_tree(X_right, y_right, depth + 1)\n",
    "        return node\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.n_classes = len(np.unique(y))\n",
    "        self.n_features = X.shape[1]\n",
    "        self.tree = self._grow_tree(X, y)\n",
    "\n",
    "    def _predict(self, inputs):\n",
    "        node = self.tree\n",
    "        while node.left:\n",
    "            if inputs[node.feature_index] <= node.threshold:\n",
    "                node = node.right\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node.predicted_class\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict(inputs) for inputs in X])\n",
    "\n",
    "dt = DecisionTree(max_depth = 10)\n",
    "dt.fit(X_train, y_train)\n",
    "y_val_pred = dt.predict(X_val)\n",
    "y_test_pred = dt.predict(X_test)\n",
    "print(\"Validation_accuracy:\", np.mean(y_val_pred == y_val))\n",
    "print(\"Test accuracy:\", np.mean(y_test_pred == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "69e670d9-4e23-40b2-a3da-79032d7260d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluatong Linear regression ---\n",
      "Validation Accuracy: 0.9864864864864865\n",
      "Test Accuracy: 0.9605263157894737\n",
      "Validation confusion matrix:\n",
      "[[19  0]\n",
      " [ 1 54]]\n",
      "Test Confusion matrix\n",
      "[[20  0]\n",
      " [ 3 53]]\n",
      "Validation - Precision: 1.000, Recal: 0.982, F1_score: 0.991\n",
      "Test - Precision: 1.000, Recal: 0.946, F1_score: 0.972\n",
      "Predicted class counts (test):\n",
      "Class 0: 23 predictions\n",
      "Class 1: 53 predictions\n",
      "\n",
      "--- Evaluatong Logistic regression ---\n",
      "Validation Accuracy: 0.9864864864864865\n",
      "Test Accuracy: 1.0\n",
      "Validation confusion matrix:\n",
      "[[19  0]\n",
      " [ 1 54]]\n",
      "Test Confusion matrix\n",
      "[[20  0]\n",
      " [ 0 56]]\n",
      "Validation - Precision: 1.000, Recal: 0.982, F1_score: 0.991\n",
      "Test - Precision: 1.000, Recal: 1.000, F1_score: 1.000\n",
      "Predicted class counts (test):\n",
      "Class 0: 20 predictions\n",
      "Class 1: 56 predictions\n",
      "\n",
      "--- Evaluatong KNN ---\n",
      "Validation Accuracy: 0.9864864864864865\n",
      "Test Accuracy: 0.9736842105263158\n",
      "Validation confusion matrix:\n",
      "[[19  0]\n",
      " [ 1 54]]\n",
      "Test Confusion matrix\n",
      "[[18  2]\n",
      " [ 0 56]]\n",
      "Validation - Precision: 1.000, Recal: 0.982, F1_score: 0.991\n",
      "Test - Precision: 0.966, Recal: 1.000, F1_score: 0.982\n",
      "Predicted class counts (test):\n",
      "Class 0: 18 predictions\n",
      "Class 1: 58 predictions\n",
      "\n",
      "--- Evaluatong Decision tree ---\n",
      "Validation Accuracy: 0.7432432432432432\n",
      "Test Accuracy: 0.7368421052631579\n",
      "Validation confusion matrix:\n",
      "[[ 0 19]\n",
      " [ 0 55]]\n",
      "Test Confusion matrix\n",
      "[[ 0 20]\n",
      " [ 0 56]]\n",
      "Validation - Precision: 0.743, Recal: 1.000, F1_score: 0.853\n",
      "Test - Precision: 0.737, Recal: 1.000, F1_score: 0.848\n",
      "Predicted class counts (test):\n",
      "Class 1: 76 predictions\n"
     ]
    }
   ],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "def confusion_matrix(y_true, y_pred, n_classes = None):\n",
    "    y_true = y_true.astype(int)\n",
    "    y_pred = y_pred.astype(int)\n",
    "    if n_classes is None:\n",
    "        n_classes = len(np.unique(np.concatenate([y_true, y_pred])))\n",
    "    cm = np.zeros((n_classes, n_classes), dtype = int)\n",
    "    for t, p in zip(y_true, y_pred):\n",
    "        cm[t, p] += 1\n",
    "    return cm\n",
    "\n",
    "def precision_recall_f1(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    TP = cm[1, 1]\n",
    "    FP = cm[0, 1]\n",
    "    FN = cm[1, 0]\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    f1 = 2* precision * recall/ (precision + recall)\n",
    "    return precision, recall, f1\n",
    "def class_counts(y_pred):\n",
    "    classes, counts = np.unique(y_pred, return_counts=True)\n",
    "    for c, cnt in zip(classes, counts):\n",
    "        print(f\"Class {c}: {cnt} predictions\")\n",
    "\n",
    "        \n",
    "y_train = y_train.astype(int)\n",
    "y_val   = y_val.astype(int)\n",
    "y_test  = y_test.astype(int)\n",
    "\n",
    "models = {\n",
    "    \"Linear regression\": linreg,\n",
    "    \"Logistic regression\": model,\n",
    "    \"KNN\": knn,\n",
    "    \"Decision tree\": dt\n",
    "}\n",
    "\n",
    "for name, clf in models.items():\n",
    "    print(f\"\\n--- Evaluatong {name} ---\")\n",
    "    y_val_pred = clf.predict(X_val)\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "\n",
    "    if y_val_pred.dtype.kind == 'f':\n",
    "        y_val_pred = (y_val_pred >= 0.5).astype(int)\n",
    "    if y_test_pred.dtype.kind == 'f':\n",
    "        y_test_pred = (y_test_pred >= 0.5).astype(int)\n",
    "\n",
    "    print(\"Validation Accuracy:\", accuracy(y_val, y_val_pred))\n",
    "    print(\"Test Accuracy:\", accuracy(y_test, y_test_pred))\n",
    "\n",
    "    print(\"Validation confusion matrix:\")\n",
    "    print(confusion_matrix(y_val, y_val_pred))\n",
    "    print(\"Test Confusion matrix\")\n",
    "    print(confusion_matrix(y_test, y_test_pred))\n",
    "\n",
    "    val_prec, val_rec, val_f1 = precision_recall_f1(y_val, y_val_pred)\n",
    "    test_prec, test_rec, test_f1 = precision_recall_f1(y_test, y_test_pred)\n",
    "    print(f\"Validation - Precision: {val_prec:.3f}, Recal: {val_rec:.3f}, F1_score: {val_f1:.3f}\")\n",
    "    print(f\"Test - Precision: {test_prec:.3f}, Recal: {test_rec:.3f}, F1_score: {test_f1:.3f}\")\n",
    "    print(\"Predicted class counts (test):\")\n",
    "    class_counts(y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37105b07-3491-4516-a0cf-623e1b92b5a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
