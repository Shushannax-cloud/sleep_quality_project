# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z-Rh_LHr5yW35QJZB7Ry9CTxyngQObH1
"""

import kagglehub
import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score

class CustomKNNRegressor:
    def __init__(self, n_neighbors=5):
        self.n_neighbors = n_neighbors

    def fit(self, X, y):
        self.X_train = X.values
        self.y_train = y.values

    def _euclidean_distance(self, x1, x2):
        return np.sqrt(np.sum((x1 - x2)**2))

    def predict(self, X_test):
        predictions = []
        X_test_vals = X_test.values
        for x_test in X_test_vals:
            distances = []

            for x_train, y_train in zip(self.X_train, self.y_train):
                dist = self._euclidean_distance(x_test, x_train)
                distances.append((dist, y_train))

            distances.sort(key=lambda x: x[0])
            neighbors = distances[:self.n_neighbors]

            neighbor_targets = [neighbor[1] for neighbor in neighbors]
            prediction = np.mean(neighbor_targets)
            predictions.append(prediction)

        return np.array(predictions)

class CustomLinearRegression:
    def __init__(self):
        self.weights = None

    def fit(self, X, y):
        X_b = np.c_[np.ones((X.shape[0], 1)), X.values]
        X_T_X = X_b.T @ X_b
        X_T_X_inv = np.linalg.pinv(X_T_X)
        X_T_y = X_b.T @ y.values

        self.weights = X_T_X_inv @ X_T_y

    def predict(self, X_test):
        X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test.values]
        return X_test_b @ self.weights


class CustomLogisticRegression:
    def __init__(self, learning_rate=0.01, n_iterations=1000):
        self.lr = learning_rate
        self.n_iterations = n_iterations
        self.weights = None
        self.bias = None

    def _sigmoid(self, z):
        z = np.asarray(z, dtype=np.float64)
        return 1 / (1 + np.exp(-z))

    def fit(self, X, y):
        n_samples, n_features = X.shape
        y_vals = y.values.astype(np.float64)
        X_vals = X.values.astype(np.float64)

        self.weights = np.zeros(n_features, dtype=np.float64)
        self.bias = 0.0

        for _ in range(self.n_iterations):
            linear_model = X_vals @ self.weights + self.bias
            predictions = self._sigmoid(linear_model)

            dw = (1/n_samples) * (X_vals.T @ (predictions - y_vals))
            db = (1/n_samples) * np.sum(predictions - y_vals)

            self.weights -= self.lr * dw
            self.bias -= self.lr * db

    def predict(self, X_test):
        X_test_vals = X_test.values.astype(np.float64)
        linear_model = X_test_vals @ self.weights + self.bias
        y_predicted = self._sigmoid(linear_model)

        return y_predicted


class Node:
    def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None):
        self.feature_index = feature_index
        self.threshold = threshold
        self.left = left
        self.right = right
        self.value = value

class CustomDecisionTreeRegressor:
    def __init__(self, max_depth=10, min_samples_split=2):
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.root = None

    def _calculate_mse(self, y):
        if len(y) == 0:
            return 0
        return np.mean((y - np.mean(y))**2)

    def _find_best_split(self, X, y):
        best_gain = -float('inf')
        best_split = None
        n_features = X.shape[1]
        current_mse = self._calculate_mse(y)

        for feature_index in range(n_features):
            unique_values = np.unique(X[:, feature_index])
            for threshold in unique_values:
                left_indices = np.where(X[:, feature_index] <= threshold)
                right_indices = np.where(X[:, feature_index] > threshold)

                y_left, y_right = y[left_indices], y[right_indices]

                if len(y_left) == 0 or len(y_right) == 0:
                    continue

                n_total = len(y)
                mse_left, mse_right = self._calculate_mse(y_left), self._calculate_mse(y_right)

                new_mse = (len(y_left) / n_total) * mse_left + (len(y_right) / n_total) * mse_right
                gain = current_mse - new_mse

                if gain > best_gain:
                    best_gain = gain
                    best_split = {
                        'feature_index': feature_index,
                        'threshold': threshold,
                        'left_indices': left_indices,
                        'right_indices': right_indices
                    }

        return best_split

    def _build_tree(self, X, y, depth=0):
        n_samples = len(X)

        if depth >= self.max_depth or n_samples < self.min_samples_split or self._calculate_mse(y) == 0:
            leaf_value = np.mean(y)
            return Node(value=leaf_value)

        best_split = self._find_best_split(X, y)

        if best_split is None:
            leaf_value = np.mean(y)
            return Node(value=leaf_value)

        X_left, y_left = X[best_split['left_indices']], y[best_split['left_indices']]
        X_right, y_right = X[best_split['right_indices']], y[best_split['right_indices']]

        left_node = self._build_tree(X_left, y_left, depth + 1)
        right_node = self._build_tree(X_right, y_right, depth + 1)

        return Node(
            feature_index=best_split['feature_index'],
            threshold=best_split['threshold'],
            left=left_node,
            right=right_node
        )

    def fit(self, X, y):
        self.root = self._build_tree(X.values, y.values)

    def _traverse_tree(self, x, node):
        if node.value is not None:
            return node.value

        feature_value = x[node.feature_index]
        if feature_value <= node.threshold:
            return self._traverse_tree(x, node.left)
        else:
            return self._traverse_tree(x, node.right)

    def predict(self, X_test):
        return np.array([self._traverse_tree(x, self.root) for x in X_test.values])


path = kagglehub.dataset_download("varishabatool/disorder")
file_name = "Sleep_health_and_lifestyle_dataset.csv"
file_path = os.path.join(path, file_name)

df = pd.read_csv(file_path)


df.drop(['Sleep Disorder', 'Person ID'], axis=1, inplace=True)

df[['Systolic BP', 'Diastolic BP']] = df['Blood Pressure'].str.split('/', expand=True).astype(float)
df.drop('Blood Pressure', axis=1, inplace=True)

TARGET_COLUMN = 'Quality of Sleep'
NUMERIC_FEATURES = ['Age', 'Sleep Duration', 'Physical Activity Level', 'Stress Level', 'Heart Rate', 'Daily Steps', 'Systolic BP', 'Diastolic BP']

scaler = StandardScaler()
df[NUMERIC_FEATURES] = scaler.fit_transform(df[NUMERIC_FEATURES])

X = df.drop(TARGET_COLUMN, axis=1)
y = df[TARGET_COLUMN]

X = pd.get_dummies(X, drop_first=True)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

plt.figure(figsize=(8, 6))
sns.histplot(y, kde=True, bins=len(y.unique()), color='skyblue')
plt.title(f'Target Distribution ({TARGET_COLUMN})')
plt.show()

knn_model = CustomKNNRegressor(n_neighbors=5)
knn_model.fit(X_train, y_train)
y_pred_knn = knn_model.predict(X_test)


dt_regressor = CustomDecisionTreeRegressor(max_depth=5)
dt_regressor.fit(X_train, y_train)
y_pred_dt = dt_regressor.predict(X_test)


poly = PolynomialFeatures(degree=2)
X_train_poly = poly.fit_transform(X_train)
X_test_poly = poly.transform(X_test)

X_train_poly_df = pd.DataFrame(X_train_poly, columns=[f'poly_{i}' for i in range(X_train_poly.shape[1])])
X_test_poly_df = pd.DataFrame(X_test_poly, columns=[f'poly_{i}' for i in range(X_test_poly.shape[1])])
y_train_reset = y_train.reset_index(drop=True)

polyreg_model = CustomLinearRegression()
polyreg_model.fit(X_train_poly_df, y_train_reset)
y_pred_poly = polyreg_model.predict(X_test_poly_df)


logreg_model = CustomLogisticRegression()
logreg_model.fit(X_train, y_train.round())
y_pred_logreg = logreg_model.predict(X_test)


def print_regression_metrics(model_name, y_true, y_pred):
    mse = mean_squared_error(y_true, y_pred)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)

    print(f"\n--- Model Evaluation ({model_name}) ---")
    print(f"Mean Squared Error (MSE): {mse:.4f}")
    print(f"Mean Absolute Error (MAE): {mae:.4f}")
    print(f"R-squared (R2 Score): {r2:.4f}")


print_regression_metrics("K-Nearest Neighbors (Custom)", y_test, y_pred_knn)
print_regression_metrics("Decision Tree Regressor (Custom)", y_test, y_pred_dt)
print_regression_metrics("Polynomial Regression (Custom Linear Reg)", y_test, y_pred_poly)
print_regression_metrics("Logistic Regression (Custom, Probability)", y_test, y_pred_logreg)

plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred_knn, color='blue', alpha=0.6, label='KNN Predictions')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='Perfect Fit')
plt.title('KNN: True Values vs. Predictions')
plt.xlabel('True Quality of Sleep')
plt.ylabel('Predicted Quality of Sleep')
plt.legend()
plt.show()